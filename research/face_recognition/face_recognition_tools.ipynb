{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMhFJJ1o3oPE/gbnG8Lopf9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3gfUzyUnBSn6"},"source":["# Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXun2CWaShxI","executionInfo":{"status":"ok","timestamp":1625754666579,"user_tz":-180,"elapsed":22699,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"17391959-1109-4a97-f31f-c8ebe1862ee6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fckrSXRIUgkO"},"source":["!cp -r \"/content/drive/My Drive/ml/edyo/project\" .\n","!unzip /content/project/dataset/lfw-deepfunneled.zip -d /content/project/dataset/lfw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvM88hD942Vd","executionInfo":{"status":"ok","timestamp":1625755122262,"user_tz":-180,"elapsed":109,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"eb39e891-9e85-444f-a499-07f0cd9cc2b2"},"source":["cd /content/project"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R1SyaZEMVFMJ"},"source":["!pip install face_recognition\n","!pip install faiss-cpu\n","!pip install insightface==0.3.5\n","!pip install onnxruntime\n","!pip install onnxruntime-gpu\n","!pip install mxnet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDqcCH4r5bDQ","executionInfo":{"status":"ok","timestamp":1625755192254,"user_tz":-180,"elapsed":4,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}}},"source":["output_path = \"/content/drive/My Drive/ml/edyo/project/output\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AvPnErOBPDH","executionInfo":{"status":"ok","timestamp":1625755198831,"user_tz":-180,"elapsed":6579,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"1c31fc88-11f8-4bd0-e255-306506067bf1"},"source":["import os\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","\n","import faiss\n","\n","import utils.video_reader\n","import utils.recognizers\n","import importlib\n","importlib.reload(utils.video_reader)\n","importlib.reload(utils.recognizers)\n","\n","from utils.video_reader import VideoReader\n","from utils.recognizers import FaceRecognizer, InsightFaceRecognizer\n","\n","import glob\n","import time\n","import tqdm\n","import sys\n","\n","import dlib\n","dlib.DLIB_USE_CUDA"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"UYYWzm5fXf2-"},"source":["# Build index"]},{"cell_type":"code","metadata":{"id":"lQditBQ_5RRu","executionInfo":{"status":"ok","timestamp":1625755274811,"user_tz":-180,"elapsed":265,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}}},"source":["def create_index(index_path, embeddings_path, recognizer, dimensions=512):\n","    embeddings = []\n","    failed = []\n","    emdeddings_df = pd.DataFrame([], columns=[\"person_name\", \"photo_path\"])\n","\n","    for person_name in tqdm.tqdm(os.listdir(lfw_photo), file=sys.stdout):\n","        images_path = f\"{lfw_photo}/{person_name}\"\n","        for image_name in os.listdir(images_path):\n","            photo_path = f\"{images_path}/{image_name}\"\n","            try:\n","                image = cv2.imread(photo_path)[:,:,::-1]\n","                encoding = recognizer.get_encodings(image)\n","\n","                embeddings.append(recognizer.get_face_embedding(encoding))\n","                emdeddings_df = emdeddings_df.append({\"person_name\":person_name, \"photo_path\":photo_path}, ignore_index=True)\n","\n","            except Exception as e:\n","                failed.append(f\"{photo_path}\")\n","\n","    embeddings = np.array(embeddings, dtype=np.float32)\n","    emdeddings_df.to_csv(embeddings_path, index=False)\n","\n","    print(f\"Processed images: {len(embeddings)}\")\n","    print(f\"Failed images: {len(failed)}\")\n","\n","    index = faiss.IndexFlatL2(dimensions)\n","    index.add(embeddings)\n","    faiss.write_index(index, index_path)\n","\n","\n","def add_new_person(embeddings_path, index_path, recognizer, person_name, photo_path):\n","    embeddings_df = pd.read_csv(embeddings_path)\n","    index = faiss.read_index(index_path)\n","\n","    image = cv2.imread(photo_path)[:, :, ::-1]\n","    encoding = recognizer.get_encodings(image)\n","    try:\n","        index.add(np.array([recognizer.get_face_embedding(encoding)], dtype=np.float32))\n","        faiss.write_index(index, index_path)\n","        \n","        embeddings_df = embeddings_df.append({\"person_name\": person_name, \"photo_path\": photo_path}, ignore_index=True)\n","        embeddings_df.to_csv(embeddings_path, index=False)\n","    except ValueError as e:\n","        print(e)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZ7CXyNHXatA"},"source":["lfw_path = './dataset/lfw'\n","lfw_photo = f\"{lfw_path}/lfw-deepfunneled/lfw-deepfunneled\"\n","\n","photo_path = \"./dataset/gans Hanna Shubina.jpg\"\n","person_name = \"Hanna Shubina\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajSsKNCz_nVS"},"source":["## Face recognition + Faiss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_zweq-h5L6-","executionInfo":{"status":"ok","timestamp":1625681754602,"user_tz":-180,"elapsed":256568,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"e48fceb1-3ff2-40f4-9a9f-54579ca3b3a8"},"source":["embeddings_path = f\"{output_path}/face_recognition_embeddings.csv\"\n","index_path = f\"{output_path}/face_recognition_vector.index\"\n","recognizer = FaceRecognizer(ctx_id=0)\n","dimensions = 128\n","\n","create_index(index_path, embeddings_path, recognizer, dimensions=dimensions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 5749/5749 [04:16<00:00, 22.42it/s]\n","Processed images: 11884\n","Failed images: 1349\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"090U0ohp_iNB","executionInfo":{"status":"ok","timestamp":1625681897414,"user_tz":-180,"elapsed":228,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"61e645a0-1717-4566-9cd3-6fe0c8c546bf"},"source":["add_new_person(embeddings_path, index_path, recognizer, person_name, photo_path)\n","\n","embeddings_df = pd.read_csv(embeddings_path)\n","embeddings_df.iloc[-5:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>person_name</th>\n","      <th>photo_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>11880</th>\n","      <td>Daniel_Barenboim</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>11881</th>\n","      <td>Noah_Wyle</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>11882</th>\n","      <td>Noah_Wyle</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>11883</th>\n","      <td>Noah_Wyle</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>11884</th>\n","      <td>Hanna Shubina</td>\n","      <td>./dataset/gans Hanna Shubina.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            person_name                                         photo_path\n","11880  Daniel_Barenboim  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","11881         Noah_Wyle  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","11882         Noah_Wyle  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","11883         Noah_Wyle  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","11884     Hanna Shubina                   ./dataset/gans Hanna Shubina.jpg"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"2IEusfJK_rla"},"source":["## Insightface + Faiss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxM2WrQ1AJxC","executionInfo":{"status":"ok","timestamp":1625682327261,"user_tz":-180,"elapsed":424449,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"c9b5a793-6c9c-4cc8-cd43-d974019f6b27"},"source":["embeddings_path = f\"{output_path}/insightface_embeddings.csv\"\n","index_path = f\"{output_path}/insightface_vector.index\"\n","recognizer = InsightFaceRecognizer(ctx_id=1)\n","dimensions = 512\n","\n","create_index(index_path, embeddings_path, recognizer, dimensions=dimensions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model ignore: /root/.insightface/models/antelopev2/1k3d68.onnx landmark_3d_68\n","model ignore: /root/.insightface/models/antelopev2/2d106det.onnx landmark_2d_106\n","model ignore: /root/.insightface/models/antelopev2/genderage.onnx genderage\n","find model: /root/.insightface/models/antelopev2/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n","find model: /root/.insightface/models/antelopev2/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n","set det-size: (640, 640)\n","100%|██████████| 5749/5749 [07:02<00:00, 13.61it/s]\n","Processed images: 10934\n","Failed images: 2299\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"ksC98w-oAfMm","executionInfo":{"status":"ok","timestamp":1625682544331,"user_tz":-180,"elapsed":597,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"473925c5-f5bc-4ea9-bd44-529546546dbc"},"source":["add_new_person(embeddings_path, index_path, recognizer, person_name, photo_path)\n","\n","embeddings_df = pd.read_csv(embeddings_path)\n","embeddings_df.iloc[-5:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>person_name</th>\n","      <th>photo_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10930</th>\n","      <td>Daniel_Barenboim</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>10931</th>\n","      <td>Noah_Wyle</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>10932</th>\n","      <td>Noah_Wyle</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>10933</th>\n","      <td>Noah_Wyle</td>\n","      <td>./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...</td>\n","    </tr>\n","    <tr>\n","      <th>10934</th>\n","      <td>Hanna Shubina</td>\n","      <td>./dataset/gans Hanna Shubina.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            person_name                                         photo_path\n","10930  Daniel_Barenboim  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","10931         Noah_Wyle  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","10932         Noah_Wyle  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","10933         Noah_Wyle  ./dataset/lfw/lfw-deepfunneled/lfw-deepfunnele...\n","10934     Hanna Shubina                   ./dataset/gans Hanna Shubina.jpg"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"nNVYR2SeAmO8"},"source":["# Process video"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"LaT0DhyvAqUN","executionInfo":{"status":"ok","timestamp":1625755285329,"user_tz":-180,"elapsed":280,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"9e235a50-004c-4caa-b948-03f6f33abf5a"},"source":["annotations_df = pd.read_csv(f\"./dataset/Milestone 1 - Dataset.csv\")\n","annotations_df[:5]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Video</th>\n","      <th>Unnamed: 1</th>\n","      <th>Person in camera</th>\n","      <th>Unnamed: 3</th>\n","      <th>Unnamed: 4</th>\n","      <th>Light conditions</th>\n","      <th>Unnamed: 6</th>\n","      <th>Unnamed: 7</th>\n","      <th>Face</th>\n","      <th>Unnamed: 9</th>\n","      <th>Unnamed: 10</th>\n","      <th>Background</th>\n","      <th>Unnamed: 12</th>\n","      <th>Engagment</th>\n","      <th>Unnamed: 14</th>\n","      <th>Mood</th>\n","      <th>Unnamed: 16</th>\n","      <th>Unnamed: 17</th>\n","      <th>Unnamed: 18</th>\n","      <th>Unnamed: 19</th>\n","      <th>Video quality</th>\n","      <th>Unnamed: 21</th>\n","      <th>Unnamed: 22</th>\n","      <th>Noise</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>No person</td>\n","      <td>One person</td>\n","      <td>Multiple persons</td>\n","      <td>Dark</td>\n","      <td>Normal</td>\n","      <td>Light</td>\n","      <td>Visible</td>\n","      <td>Not facing camera</td>\n","      <td>Occluded</td>\n","      <td>Plain</td>\n","      <td>Messy</td>\n","      <td>Involved</td>\n","      <td>Distracted</td>\n","      <td>Bored</td>\n","      <td>Surprised</td>\n","      <td>Smiling</td>\n","      <td>Neutural</td>\n","      <td>Interested</td>\n","      <td>Low\\n(640x360)</td>\n","      <td>Normal\\n(960x540)</td>\n","      <td>High (1920x1080)</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>52.0</td>\n","      <td>3</td>\n","      <td>112</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>112</td>\n","      <td>0</td>\n","      <td>112</td>\n","      <td>15</td>\n","      <td>2</td>\n","      <td>112</td>\n","      <td>0</td>\n","      <td>112</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>112</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>112</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>video_1_0000.mp4</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>video_1_0001.mp4</td>\n","      <td>10.0</td>\n","      <td>+</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>video_1_0002.mp4</td>\n","      <td>7.0</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              Video  Unnamed: 1  ...       Unnamed: 22 Noise\n","0               NaN         NaN  ...  High (1920x1080)   NaN\n","1               NaN        52.0  ...               112   0.0\n","2  video_1_0000.mp4         9.0  ...                 +   NaN\n","3  video_1_0001.mp4        10.0  ...                 +   NaN\n","4  video_1_0002.mp4         7.0  ...                 +   NaN\n","\n","[5 rows x 24 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"I_gHVR1nAwNV","executionInfo":{"status":"ok","timestamp":1625755287042,"user_tz":-180,"elapsed":259,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"dc94ea37-4757-496a-a53d-658a929837ac"},"source":["annotated_df = annotations_df[annotations_df.iloc[:, 1].notna()]\n","face_annotations_df = annotated_df.iloc[1:, [0,2,3,4]].copy()\n","face_annotations_df.columns = [\"video_name\", \"no_face\", \"one_face\", \"multiple_faces\"]\n","face_annotations_df.reset_index(drop=True, inplace=True)\n","face_annotations_df[:5]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_name</th>\n","      <th>no_face</th>\n","      <th>one_face</th>\n","      <th>multiple_faces</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>video_1_0000.mp4</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>video_1_0001.mp4</td>\n","      <td>+</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>video_1_0002.mp4</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>video_1_0003.mp4</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>video_1_0004.mp4</td>\n","      <td>NaN</td>\n","      <td>+</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         video_name no_face one_face multiple_faces\n","0  video_1_0000.mp4     NaN        +            NaN\n","1  video_1_0001.mp4       +        +            NaN\n","2  video_1_0002.mp4     NaN        +            NaN\n","3  video_1_0003.mp4     NaN        +            NaN\n","4  video_1_0004.mp4     NaN        +            NaN"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"JiWFFZRdAzu1","executionInfo":{"status":"ok","timestamp":1625755290839,"user_tz":-180,"elapsed":261,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}}},"source":["def process_video(video_path, recognizer, skip_frames):\n","    vr = VideoReader(video_path)\n","    \n","    info_df = pd.DataFrame([], columns=[\"frame\", \"num_faces\", \"person_name\", \"distance\", \"process_time\"])\n","    while vr.has_next():\n","        frame = vr.read(skip=skip_frames, rgb=True)\n","        \n","        try:\n","            start_time = time.time()\n","            face_encodings = recognizer.get_encodings(frame)\n","            person_name = None\n","            distance = None\n","            distance, person_name = recognizer.recognize_face(face_encodings)\n","\n","            process_time = time.time() - start_time\n","            info_df = info_df.append({\"frame\": vr.get_frame_idx(), \"num_faces\": len(face_encodings),\n","                                        \"person_name\": person_name, \"distance\": distance,\n","                                        \"process_time\": process_time}, ignore_index=True)\n","        except:\n","            info_df = info_df.append({\"frame\": vr.get_frame_idx(), \"num_faces\": len(face_encodings),\n","                                      \"person_name\": None, \"distance\": None,\n","                                      \"process_time\": None}, ignore_index=True)\n","        \n","    return info_df, vr.fps, vr.video_length\n","\n","def run(recognizer, videos_list, videos_path, recognizer_prefix, output_path):\n","    video_qualities = [\"low\", \"medium\", \"high\"] #[\"low\"]#\n","    devices = [\"gpu\"] #[\"cpu\", \"gpu\"]\n","    skips = [5, 10, 20, 30] #[30]#\n","    summary_df = pd.DataFrame([], columns=[\"video_path\", \"device\", \"skip_frames\", \"video_length_s\", \"fps\", \"processed_time\", \"output_fps\"])\n","\n","    # TODO simplify\n","    for video_name in tqdm.tqdm(videos_list, file=sys.stdout):\n","        for video_quality in video_qualities:\n","            video_path = f\"{videos_path}/{video_quality}/{video_name}\"\n","\n","            for device in devices:\n","                for skip_frames in skips:\n","                    info_df, fps, video_length = process_video(video_path, recognizer, skip_frames=skip_frames)\n","                    info_df.to_csv(f\"{output_path}/{video_quality}/{recognizer_prefix}_{video_name}_{device}_{skip_frames}.csv\", index=False)\n","                    \n","                    video_length_s = video_length/fps\n","                    processed_time = info_df['process_time'].sum()\n","                    output_fps = video_length/processed_time\n","                    summary_df = summary_df.append({\"video_path\": video_path, \"device\":device, \"skip_frames\": skip_frames, \n","                                                    \"video_length_s\": video_length_s, \"fps\": fps,\n","                                                    \"processed_time\": processed_time, \"output_fps\": output_fps},\n","                                                ignore_index=True)\n","                    \n","    summary_df.to_csv(f\"{output_path}/{recognizer_prefix}_summary.csv\", index=False)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9gEKOW6A_ic","executionInfo":{"status":"ok","timestamp":1625755294078,"user_tz":-180,"elapsed":241,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"759a6df6-a9e9-4513-afab-a6f2cd472026"},"source":["os.makedirs(f\"{output_path}/low\", exist_ok=True)\n","os.makedirs(f\"{output_path}/medium\", exist_ok=True)\n","os.makedirs(f\"{output_path}/high\", exist_ok=True)\n","\n","videos_list = face_annotations_df[\"video_name\"].values\n","print(f\"{len(videos_list)} videos\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["52 videos\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DyAKHL64Mfer"},"source":["## Face recognition + Faiss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tSvOYfgMeVT","executionInfo":{"status":"ok","timestamp":1625696661206,"user_tz":-180,"elapsed":13191839,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"7c323c77-a023-424a-c437-b24fe68960e4"},"source":["embeddings_path = f\"{output_path}/face_recognition_embeddings.csv\"\n","index_path = f\"{output_path}/face_recognition_vector.index\"\n","recognizer = FaceRecognizer(embeddings_path, index_path, ctx_id=0)\n","\n","run(recognizer, videos_list, \"./dataset/video\", \"fr\", output_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","\n","  0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","  2%|▏         | 1/52 [04:13<3:35:25, 253.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  4%|▍         | 2/52 [08:21<3:29:57, 251.95s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  6%|▌         | 3/52 [12:41<3:27:32, 254.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","  8%|▊         | 4/52 [16:57<3:23:49, 254.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 10%|▉         | 5/52 [21:14<3:20:08, 255.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 12%|█▏        | 6/52 [25:29<3:15:46, 255.37s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 13%|█▎        | 7/52 [29:48<3:12:18, 256.42s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 15%|█▌        | 8/52 [34:05<3:08:15, 256.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 17%|█▋        | 9/52 [38:26<3:04:42, 257.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 19%|█▉        | 10/52 [42:39<2:59:34, 256.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 21%|██        | 11/52 [46:55<2:55:07, 256.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 23%|██▎       | 12/52 [51:13<2:51:09, 256.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 25%|██▌       | 13/52 [55:31<2:47:07, 257.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 27%|██▋       | 14/52 [59:46<2:42:25, 256.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 29%|██▉       | 15/52 [1:04:01<2:37:59, 256.21s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 31%|███       | 16/52 [1:08:17<2:33:33, 255.94s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 33%|███▎      | 17/52 [1:12:32<2:29:09, 255.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 35%|███▍      | 18/52 [1:16:41<2:23:44, 253.66s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 37%|███▋      | 19/52 [1:20:52<2:19:03, 252.85s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 38%|███▊      | 20/52 [1:25:07<2:15:16, 253.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 40%|████      | 21/52 [1:29:23<2:11:22, 254.28s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 42%|████▏     | 22/52 [1:33:39<2:07:27, 254.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 44%|████▍     | 23/52 [1:37:59<2:03:52, 256.30s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 46%|████▌     | 24/52 [1:41:44<1:55:19, 247.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 48%|████▊     | 25/52 [1:46:02<1:52:37, 250.29s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 50%|█████     | 26/52 [1:50:19<1:49:20, 252.34s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 52%|█████▏    | 27/52 [1:54:36<1:45:41, 253.67s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 54%|█████▍    | 28/52 [1:58:51<1:41:35, 253.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 56%|█████▌    | 29/52 [2:03:08<1:37:46, 255.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 58%|█████▊    | 30/52 [2:07:25<1:33:43, 255.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 60%|█████▉    | 31/52 [2:11:40<1:29:24, 255.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 62%|██████▏   | 32/52 [2:15:57<1:25:13, 255.66s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 63%|██████▎   | 33/52 [2:20:14<1:21:05, 256.09s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 65%|██████▌   | 34/52 [2:24:28<1:16:39, 255.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 67%|██████▋   | 35/52 [2:28:45<1:12:34, 256.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 69%|██████▉   | 36/52 [2:33:03<1:08:23, 256.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 71%|███████   | 37/52 [2:37:19<1:04:07, 256.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 73%|███████▎  | 38/52 [2:41:37<59:55, 256.80s/it]  \u001b[A\u001b[A\u001b[A\n","\n","\n"," 75%|███████▌  | 39/52 [2:45:53<55:37, 256.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 77%|███████▋  | 40/52 [2:50:10<51:19, 256.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 79%|███████▉  | 41/52 [2:54:21<46:46, 255.16s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 81%|████████  | 42/52 [2:58:35<42:27, 254.75s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 83%|████████▎ | 43/52 [3:02:53<38:21, 255.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 85%|████████▍ | 44/52 [3:07:11<34:09, 256.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 87%|████████▋ | 45/52 [3:11:25<29:50, 255.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 88%|████████▊ | 46/52 [3:15:42<25:36, 256.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 90%|█████████ | 47/52 [3:19:57<21:18, 255.73s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 92%|█████████▏| 48/52 [3:24:12<17:01, 255.38s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 94%|█████████▍| 49/52 [3:28:26<12:45, 255.22s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 96%|█████████▌| 50/52 [3:32:41<08:30, 255.15s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n"," 98%|█████████▊| 51/52 [3:36:49<04:12, 252.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","100%|██████████| 52/52 [3:39:51<00:00, 253.69s/it]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xuwpOVq-M1Pk"},"source":["## Insightface + Faiss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTxs8dLeMbAM","executionInfo":{"status":"ok","timestamp":1625760015440,"user_tz":-180,"elapsed":4702945,"user":{"displayName":"Hanna Shubina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh73Ho9VabLMNDASh-KacM0lV_NgHZ2i1-yTqZjCA=s64","userId":"03697660510887668517"}},"outputId":"c729d9a1-6093-43fc-e810-d8d2e4f57aa7"},"source":["embeddings_path = f\"{output_path}/insightface_embeddings.csv\"\n","index_path = f\"{output_path}/insightface_vector.index\"\n","recognizer = InsightFaceRecognizer(embeddings_path, index_path, ctx_id=0)\n","\n","run(recognizer, videos_list, \"./dataset/video\", \"if\", output_path)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["download_path: /root/.insightface/models/antelopev2\n","Downloading /root/.insightface/models/antelopev2.zip from http://storage.insightface.ai/files/models/antelopev2.zip...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 352289/352289 [00:06<00:00, 57006.66KB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["model ignore: /root/.insightface/models/antelopev2/1k3d68.onnx landmark_3d_68\n","model ignore: /root/.insightface/models/antelopev2/2d106det.onnx landmark_2d_106\n","model ignore: /root/.insightface/models/antelopev2/genderage.onnx genderage\n","find model: /root/.insightface/models/antelopev2/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n","find model: /root/.insightface/models/antelopev2/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n","set det-size: (640, 640)\n","100%|██████████| 52/52 [1:18:06<00:00, 90.13s/it]\n"],"name":"stdout"}]}]}